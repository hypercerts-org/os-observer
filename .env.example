# .env
## This .env file is mostly used for Python data ops

## Google Cloud setup
GOOGLE_PROJECT_ID=
# You will need to generate Google application credentials
# Note: You can use your gcloud auth credentials
GOOGLE_APPLICATION_CREDENTIALS=<path-to-valid-gcp-creds>
# Used for data transfer between databases
CLOUDSTORAGE_BUCKET_NAME=
# Used for storing all BigQuery data in the dbt pipeline
BIGQUERY_DATASET_ID=
# Used for Frontend/API-facing services
CLOUDSQL_REGION=
CLOUDSQL_INSTANCE_ID=
CLOUDSQL_DB_NAME=
CLOUDSQL_DB_PASSWORD=
CLOUDSQL_DB_USER=

## Dagster Setup
# You may want to change the location of dagster home if you want it to survive resets 
DAGSTER_HOME=/tmp/dagster-home 
# This is used to put generated dbt profiles for dagster in a specific place
DAGSTER_DBT_TARGET_BASE_DIR=/tmp/dagster-home/generated-dbt
DAGSTER_DBT_PARSE_PROJECT_ON_LOAD=1

# Used when loading dlt assets into a staging area. It should be set to a GCS
# bucket that will be used to write to for dlt data transfers into bigquery.
DAGSTER_STAGING_BUCKET_URL=some-bucket

# Uncomment the next two vars to use gcp secrets (you'll need to have gcp
# secrets configured). Unfortunately at this time, if you don't have access to
# the official oso gcp account uncommenting these will likely not work. The GCP
# secrets prefix should likely match the dagster deployment's search prefix in
# flux
#DAGSTER_USE_LOCAL_SECRETS=False
#DAGSTER_GCP_SECRETS_PREFIX=dagster 