---
title: Run SQL Queries
sidebar_position: 3
---

import Button from "../../src/components/plasmic/Button";

As part of our
[open source, open data, open infrastructure](../../blog/open-source-open-data-open-infra)
initiative, we are making OSO data as widely available as possible.
Use this guide to download the latest data for our own data stack.

Please refer to the
[getting started](../get-started/index.mdx)
guide to first get your BigQuery account setup.

## Subscribe an OSO dataset

First, we need to subscribe to an OSO dataset in your own
Google Cloud account. 
You can see all of our available datasets in the
[Data Overview](./overview/index.mdx).

We recommend starting with the OSO production data pipeline here:

<Button
  size={"compact"}
  color={"blue"}
  target={"_blank"}
  link={"https://console.cloud.google.com/bigquery/analytics-hub/exchanges/projects/87806073973/locations/us/dataExchanges/open_source_observer_190181416ae/listings/oso_data_pipeline_190187c6517"}
  children={"Subscribe on BigQuery"}
/>

After subscribing, you can reference the dataset
within your GCP project namespace, for example:
`YOUR_PROJECT_NAME.oso_production`


## Cost Estimation

BigQuery [on-demand pricing](https://cloud.google.com/bigquery/pricing)
charges by the number of bytes scanned,
with the first 1 TB free every month.

Therefore, it is helpful to keep track of how many bytes you will be
scanning before running queries.

![cost estimate](./bigquery_cost_estimate.png)

As long as the query is valid, you should see the
bytes scanned in the top right corner
before running your query.

## Exploring the data

The OSO data peipleine is fully visible to queries.
You can find all of the model definitions under
[`warehouse/dbt/models/`](https://github.com/opensource-observer/oso/tree/main/warehouse/dbt/models)
in our
[monorepo](https://github.com/opensource-observer/oso).

We also maintain reference documentation at
[https://models.opensource.observer/](https://models.opensource.observer/),
where you can find the model 
[lineage graph](https://models.opensource.observer/#!/overview?g_v=1).
These references can help you understand the schema 
of any particular model to form your queries.

Generally speaking there are three types of models:
1. **Staging models and source data**:
For each data source, staging models are created to clean and normalize
the necessary subset of data.
2. **Intermediate models**:
Here, we join all data sources into a master event table,
[`int_events`](https://models.opensource.observer/#!/model/model.opensource_observer.int_events).
Then, we produce a series of aggregations such as 
[`int_events_daily_to_project`](https://models.opensource.observer/#!/model/model.opensource_observer.int_events_daily_to_project)
3. **Mart models**:
From the intermediate models, we create the final metrics models
that are served from the API.

## Cost optimization

Generally speaking, downstream models are typically smaller
than upstream models, like source data.
Therefore, it is generally recommended to use the model
that is furtherest downstream in the lineage graph
that can satisfy your query.
Each stage of the pipeline typically reduces the size of the data
by 1-2 orders of magnitude.

If there is an intermediate model addition
(such as a new event type or aggregation)
that you think can help save costs for others in the future,
please consider contributing to our 
[data models](../contribute/impact-models.md).
